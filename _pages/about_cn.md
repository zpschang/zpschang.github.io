---
layout: about
title: 关于我
permalink: /cn/
lang: cn
# subtitle: Researcher at Microsoft Research Asia. 

# profile:
#   align: right
#   image: prof_pic.jpg
#   image_circular: false # crops the image to make it circular
#   more_info: >
#     111

# news: true  # includes a list of news items
# latest_posts: false  # includes a list of the newest posts
# selected_papers: false # includes a list of papers marked as "selected={true}"
# social: true  # includes social icons at the bottom of the page
---

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.  -->

**张蒲石**  
*研究员 & AI工程师*

[Google Scholar](https://scholar.google.com/citations?user=_DLMSkIAAAAJ&hl=en&oi=ao) \| 邮箱: [zpschang@gmail.com](mailto:zpschang@gmail.com) \| Github: [zpschang](https://github.com/zpschang)

## 关于我

我是一名研究员和AI工程师，专注于开发能够感知、理解并与物理世界交互的智能机器人系统。

目前，我在自变量机器人工作，专注于构建先进的机器人系统，致力于弥合AI研究与实际应用之间的差距。

此前，我曾是微软亚洲研究院[赵立](https://www.microsoft.com/en-us/research/people/lizo/)研究员团队的一员，负责与微软Xbox团队合作的基于视觉的游戏测试AI项目和VLA模型的研究。

## 最新动态

- **2025年9月**: 发布了[PIG-Nav](https://github.com/zpschang/PIG-Nav)的代码和模型，在图像目标导航任务中取得了优异的性能
- **2025年8月**: 发布了[Villa-X](https://microsoft.github.io/villa-x/)：增强视觉-语言-动作模型中的潜在动作建模
- **2024年10月**: 发布了[IGOR](https://www.microsoft.com/en-us/research/project/igor-image-goal-representations/)：图像目标表示作为具身AI基础模型的原子控制单元


## 主要论文

### 2025年
- Villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models
  *arXiv 2025*  
  [[PDF](https://arxiv.org/pdf/2507.23682)] [[Code](https://microsoft.github.io/villa-x/)] [[Website](https://microsoft.github.io/villa-x/)]

- PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models
  *arXiv 2025*  
  [[PDF](https://arxiv.org/pdf/2507.17220)] [[Demo](https://youtu.be/y6Eu7EVLhKA)] [[Code](https://github.com/zpschang/PIG-Nav)]

- What Do Latent Action Models Actually Learn?
  *NeurIPS 2025*  
  [[PDF](https://arxiv.org/pdf/2507.17220)]

### 2023年
- IG-Net: Image-Goal Network for Offline Visual Navigation on A Large-Scale Game Map
  Baiting Zhu\*, **Pushi Zhang**\*, Xin-Qiang Cai\*, Li Zhao, Masashi Sugiyama, Jiang Bian  
  *NeurIPS 2023 Robot Learning Workshop*  
  [[PDF](https://www.robot-learning.ml/2023/files/paper32.pdf)] [[Demo](https://www.youtube.com/watch?v=pOtnB_Rfciw)]

- Distributional Pareto-Optimal Multi-Objective Reinforcement Learning 
  Xin-Qiang Cai\*, **Pushi Zhang**\*, Li Zhao, Jiang Bian, Masashi Sugiyama, Ashley J. Llorens  
  *NeurIPS 2023*  
  [[PDF](https://papers.nips.cc/paper_files/paper/2023/file/32285dd184dbfc33cb2d1f0db53c23c5-Paper-Conference.pdf)] [[Code](https://github.com/zpschang/DPMORL)]

- Asking Before Action: Gather Information in Embodied Decision Making with Language Models
  Xiaoyu Chen, Shenao Zhang, **Pushi Zhang**, Li Zhao, Jianyu Chen  
  *arXiv 2023*  
  [[PDF](https://arxiv.org/pdf/2305.15695)]

### 2022年
- An Adaptive Deep RL Method for Non-Stationary Environments with Piecewise Stable Context
  Xiaoyu Chen, Xiangming Zhu, Yufeng Zheng, **Pushi Zhang**, Li Zhao, Wenxue Cheng, Peng Cheng, Yongqiang Xiong, Tao Qin, Jianyu Chen, Tie-Yan Liu  
  *NeurIPS 2022*  
  [[PDF](https://arxiv.org/pdf/2212.12735)]

### 2021年
- Distributional Reinforcement Learning for Multi-Dimensional Reward Functions
  **Pushi Zhang**, Xiaoyu Chen, Li Zhao, Wei Xiong, Tao Qin, Tie-Yan Liu  
  *NeurIPS 2021*  
  [[PDF](https://proceedings.neurips.cc/paper/2021/file/0b9e57c46de934cee33b0e8d1839bfc2-Paper.pdf)] [[Code](https://github.com/zpschang/MD3QN)]

- Independence-aware Advantage Estimation
  **Pushi Zhang**, Li Zhao, Guoqing Liu, Jiang Bian, Minlie Huang, Tao Qin, Tie-Yan Liu  
  *IJCAI 2021*  
  [[PDF](https://www.ijcai.org/proceedings/2021/0461.pdf)]

- Demonstration Actor Critic
  Guoqing Liu, Li Zhao, **Pushi Zhang**, Jiang Bian, Tao Qin, Nenghai Yu, Tie-Yan Liu  
  *Neurocomputing 2021*  
  [[PDF](https://www.sciencedirect.com/science/article/abs/pii/S0925231220320282)]

 
## 关于我自己

我希望通过进行有意义的研究来对社会产生积极影响。我的目标是创造有益于世界的知识和解决方案，并在此过程中激励他人。

---

*最后更新：2025年10月5日*

<!-- ## Our environment for Visual Navigation -->
<!-- <blockquote>
We must perceive in order to move, but we must also move in order to perceive. 
</blockquote> -->

<!-- <blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
</blockquote> -->

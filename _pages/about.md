---
layout: about
title: About
permalink: /
# subtitle: Researcher at Microsoft Research Asia. 

# profile:
#   align: right
#   image: prof_pic.jpg
#   image_circular: false # crops the image to make it circular
#   more_info: >
#     111

# news: true  # includes a list of news items
# latest_posts: false  # includes a list of the newest posts
# selected_papers: false # includes a list of papers marked as "selected={true}"
# social: true  # includes social icons at the bottom of the page
---

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.  -->

*Researcher & AI Engineer*

[Google Scholar](https://scholar.google.com/citations?user=_DLMSkIAAAAJ&hl=en&oi=ao) \| Email: [zpschang@gmail.com](mailto:zpschang@gmail.com) \| Github: [zpschang](https://github.com/zpschang)

## About Me

I am a researcher and AI engineer passionate about developing intelligent robotics systems that can perceive, understand, and interact with the physical world. Currently, I work at **X Square Robot**, where I focus on building advanced robotics systems that bridge the gap between AI research and real-world applications. 

Previously, I was a researcher at **Microsoft Research Asia** in [Li Zhao](https://www.microsoft.com/en-us/research/people/lizo/)'s group, where I led the Vision-based Game-Testing AI project in collaboration with the Microsoft Xbox team. 

## Recent News

- **Aug 2025**: Released [**Villa-X**](https://microsoft.github.io/villa-x/): Enhancing Latent Action Modeling in Vision-Language-Action Models
- **Oct 2024**: Launched [**IGOR**](https://www.microsoft.com/en-us/research/project/igor-image-goal-representations/): Image-Goal Representations as Atomic Control Units for Foundation Models in Embodied AI
- **Nov 2023**: Two papers accepted at NeurIPS 2023:
  - [**Distributional Pareto-Optimal Multi-Objective Reinforcement Learning**](https://openreview.net/pdf?id=prIwYTU9PV) (NeurIPS 2023)
  - [**IG-Net: Image-Goal Network for Offline Visual Navigation**](https://www.robot-learning.ml/2023/files/paper32.pdf) (NeurIPS Robot Learning Workshop) 


## Selected Publications

- **Villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**  
  *arXiv 2025*  
  [[PDF](https://arxiv.org/pdf/2507.23682)] [[Code](https://microsoft.github.io/villa-x/)] [[Website](https://microsoft.github.io/villa-x/)]

- **PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models**  
  *arXiv 2025*  
  [[PDF](https://arxiv.org/pdf/2507.17220)] [[Demo](https://youtu.be/y6Eu7EVLhKA)] [[Code](https://github.com/zpschang/PIG-Nav)]

- **What Do Latent Action Models Actually Learn?**  
  *NeurIPS 2025*  
  [[PDF](https://arxiv.org/pdf/2507.17220)]

- **IG-Net: Image-Goal Network for Offline Visual Navigation on A Large-Scale Game Map**  
  Baiting Zhu\*, **Pushi Zhang**\*, Xin-Qiang Cai\*, Li Zhao, Masashi Sugiyama, Jiang Bian  
  *NeurIPS 2023 Robot Learning Workshop*  
  [[PDF](https://www.robot-learning.ml/2023/files/paper32.pdf)] [[Demo](https://www.youtube.com/watch?v=pOtnB_Rfciw)]

- **Distributional Pareto-Optimal Multi-Objective Reinforcement Learning**  
  Xin-Qiang Cai\*, **Pushi Zhang**\*, Li Zhao, Jiang Bian, Masashi Sugiyama, Ashley J. Llorens  
  *NeurIPS 2023*  
  [[PDF](https://papers.nips.cc/paper_files/paper/2023/file/32285dd184dbfc33cb2d1f0db53c23c5-Paper-Conference.pdf)] [[Code](https://github.com/zpschang/DPMORL)]

- **Asking Before Action: Gather Information in Embodied Decision Making with Language Models**  
  Xiaoyu Chen, Shenao Zhang, **Pushi Zhang**, Li Zhao, Jianyu Chen  
  *arXiv 2023*  
  [[PDF](https://arxiv.org/pdf/2305.15695)]

- **An Adaptive Deep RL Method for Non-Stationary Environments with Piecewise Stable Context**  
  Xiaoyu Chen, Xiangming Zhu, Yufeng Zheng, **Pushi Zhang**, Li Zhao, Wenxue Cheng, Peng Cheng, Yongqiang Xiong, Tao Qin, Jianyu Chen, Tie-Yan Liu  
  *NeurIPS 2022*  
  [[PDF](https://arxiv.org/pdf/2212.12735)]

- **Distributional Reinforcement Learning for Multi-Dimensional Reward Functions**  
  **Pushi Zhang**, Xiaoyu Chen, Li Zhao, Wei Xiong, Tao Qin, Tie-Yan Liu  
  *NeurIPS 2021*  
  [[PDF](https://proceedings.neurips.cc/paper/2021/file/0b9e57c46de934cee33b0e8d1839bfc2-Paper.pdf)] [[Code](https://github.com/zpschang/MD3QN)]

- **Independence-aware Advantage Estimation**  
  **Pushi Zhang**, Li Zhao, Guoqing Liu, Jiang Bian, Minlie Huang, Tao Qin, Tie-Yan Liu  
  *IJCAI 2021*  
  [[PDF](https://www.ijcai.org/proceedings/2021/0461.pdf)]

- **Demonstration Actor Critic**  
  Guoqing Liu, Li Zhao, **Pushi Zhang**, Jiang Bian, Tao Qin, Nenghai Yu, Tie-Yan Liu  
  *Neurocomputing 2021*  
  [[PDF](https://www.sciencedirect.com/science/article/abs/pii/S0925231220320282)]


## About Myself

I hope to make a positive impact by conducting research that contributes meaningfully to society. My goal is to create knowledge and solutions that benefit the world and inspire others along the way.

---

*Last update: 2025.10.5*

<!-- ## Our environment for Visual Navigation -->
<!-- <blockquote>
We must perceive in order to move, but we must also move in order to perceive. 
</blockquote> -->

<!-- <blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    â€”Anais Nin
</blockquote> -->
